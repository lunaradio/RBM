{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fcn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Ising spins and Calculating Hamiltonian of the Ising system\n",
    "class Ising():\n",
    "    \n",
    "    def __init__(self, nRow, nCol):\n",
    "        self.spins = torch.zeros(nRow, nCol)\n",
    "        self.probs = torch.rand(nRow, nCol)\n",
    "        for i in range(nRow):\n",
    "            for j in range(nCol):\n",
    "                if self.probs[i][j] < 0.5:\n",
    "                    self.spins[i][j] = 1\n",
    "                else:\n",
    "                    self.spins[i][j] = -1\n",
    "    \n",
    "    def Hamiltonian(self):\n",
    "        H = 0.\n",
    "        J = 1.\n",
    "        nRow = self.spins.size()[0]\n",
    "        nCol = self.spins.size()[1]\n",
    "        for i in range(nRow):\n",
    "            for j in range(nCol):\n",
    "                if i < 1:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i+1][j]\n",
    "                elif i > nRow - 2:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i-1][j]\n",
    "                else:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i+1][j]\n",
    "                    H -= J * self.spins[i][j] * self.spins[i-1][j]\n",
    "                \n",
    "                if j < 1:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i][j+1]\n",
    "                elif j > nCol - 2:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i][j-1]\n",
    "                else:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i][j+1]\n",
    "                    H -= J * self.spins[i][j] * self.spins[i][j-1]\n",
    "        return H/2   #to avoid double count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to create a training data set by using Metropolis algorithm\n",
    "def Data_for_train(data_size, nRow, nCol):\n",
    "\n",
    "    for i in range(data_size):\n",
    "    \n",
    "        ising = Ising(nRow, nCol)\n",
    "        H_new = ising.Hamiltonian()\n",
    "    \n",
    "        # Reshape of a matrix of Ising spins to a vector as the visible layer\n",
    "        spin = ising.spins.view(nRow*nCol)\n",
    "        v = (1 - spin)/2   # spin 1 --> 0 ,   spin -1 --> 1\n",
    "    \n",
    "        # save visible layers as row vectors of the training data matrix\n",
    "        if i == 0:\n",
    "            data = v.unsqueeze(0)\n",
    "        else:\n",
    "            if H_new <= H:\n",
    "                data = torch.cat((data, v.unsqueeze(0)), dim = 0)\n",
    "            else:\n",
    "                B_sample = torch.bernoulli(torch.exp(H - H_new))\n",
    "                if B_sample == 1:\n",
    "                    data = torch.cat((data, v.unsqueeze(0)), dim = 0)\n",
    "                else:\n",
    "                    data = torch.cat((data, data[i-1].unsqueeze(0)), dim = 0)\n",
    "        H = H_new\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the RBM Architecture (weights, biases)\n",
    "class RBM():\n",
    "    \n",
    "    # Initiate RBM parameters\n",
    "    def __init__(self, nv, nh):\n",
    "        self.W = torch.randn(nh, nv)\n",
    "        self.a = torch.randn(nh)\n",
    "        self.b = torch.randn(nv)\n",
    "    \n",
    "    def Energy(self, v, h): \n",
    "        ah = torch.dot(self.a, h)\n",
    "        bv = torch.dot(self.b, v)\n",
    "        hWv = torch.dot(h, torch.mv(self.W, v))\n",
    "        E = - ah - bv - hWv\n",
    "        return E\n",
    "        \n",
    "    # Contrastive Divergence\n",
    "    def update_CD(self, Batch, learning_rate):\n",
    "        \n",
    "        batch_size = Batch.size()[0]\n",
    "        delta_W = torch.zeros_like(self.W)\n",
    "        delta_a = torch.zeros_like(self.a)\n",
    "        delta_b = torch.zeros_like(self.b)\n",
    "        for i in range(batch_size):\n",
    "            a = self.a\n",
    "            b = self.b\n",
    "            num_iterations = 1\n",
    "            \n",
    "            # k-iterations of Gibbs sampling\n",
    "            for k in range(num_iterations):\n",
    "                if k == 0:\n",
    "                    v_0 = Batch[i]\n",
    "                    Wv = torch.mv(self.W, v_0)\n",
    "                    p_h_given_v_0 = torch.sigmoid(a + Wv)\n",
    "                    h_0 = torch.bernoulli(p_h_given_v_0)\n",
    "                    h_old = h_0\n",
    "                else:\n",
    "                    h_old = h_new\n",
    "                \n",
    "                hW = torch.mv(torch.t(self.W), h_old)\n",
    "                p_v_given_h = torch.sigmoid(b + hW)\n",
    "                v_new = torch.bernoulli(p_v_given_h)\n",
    "                Wv = torch.mv(self.W, v_new)\n",
    "                p_h_given_v = torch.sigmoid(a + Wv)\n",
    "                h_new = torch.bernoulli(p_h_given_v)\n",
    "            \n",
    "            # update parameters after k-iterations of Gibbs sampling\n",
    "            delta_W += learning_rate * (torch.ger(p_h_given_v_0, v_0) - torch.ger(p_h_given_v, v_new)) / batch_size\n",
    "            delta_a += learning_rate * (p_h_given_v_0 - p_h_given_v) / batch_size\n",
    "            delta_b += learning_rate * (v_0 - v_new) / batch_size\n",
    "            \n",
    "        self.W += delta_W\n",
    "        self.a += delta_a\n",
    "        self.b += delta_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a training data set\n",
    "nRow = 3\n",
    "nCol = 3\n",
    "data_size = 100\n",
    "data_length = nRow*nCol\n",
    "\n",
    "D = Data_for_train(data_size, nRow, nCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RBM parameters\n",
    "nv = data_length\n",
    "nh = 10\n",
    "rbm = RBM(nv, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RBM\n",
    "num_epoch = 10\n",
    "batch_size = 10\n",
    "learning_rate = 1e-1\n",
    "\n",
    "for epoch in range(0, num_epoch + 1):\n",
    "    if epoch > 0:\n",
    "        for batch_idx in range(int(data_size / batch_size)):\n",
    "            Batch = D[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "            rbm.update_CD(Batch, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
