{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fcn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Ising spins and Calculating Hamiltonian of the Ising system\n",
    "class Ising():\n",
    "    \n",
    "    def __init__(self, nRow, nCol):\n",
    "        self.spins = torch.zeros(nRow, nCol)\n",
    "        self.probs = torch.rand(nRow, nCol)\n",
    "        for i in range(nRow):\n",
    "            for j in range(nCol):\n",
    "                if self.probs[i][j] < 0.5:\n",
    "                    self.spins[i][j] = 1\n",
    "                else:\n",
    "                    self.spins[i][j] = -1\n",
    "    \n",
    "    def Hamiltonian(self):\n",
    "        H = 0.\n",
    "        J = 1.\n",
    "        nRow = self.spins.size()[0]\n",
    "        nCol = self.spins.size()[1]\n",
    "        for i in range(nRow):\n",
    "            for j in range(nCol):\n",
    "                if i < 1:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i+1][j]\n",
    "                elif i > nRow - 2:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i-1][j]\n",
    "                else:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i+1][j]\n",
    "                    H -= J * self.spins[i][j] * self.spins[i-1][j]\n",
    "                \n",
    "                if j < 1:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i][j+1]\n",
    "                elif j > nCol - 2:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i][j-1]\n",
    "                else:\n",
    "                    H -= J * self.spins[i][j] * self.spins[i][j+1]\n",
    "                    H -= J * self.spins[i][j] * self.spins[i][j-1]\n",
    "        return H/2   #to avoid double count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the RBM Architecture (weights, biases)\n",
    "class RBM():\n",
    "    \n",
    "    # Initiate RBM parameters\n",
    "    def __init__(self, nv, nh):\n",
    "        self.W = torch.randn(nh, nv)\n",
    "        self.a = torch.randn(nh)\n",
    "        self.b = torch.randn(nv)\n",
    "    \n",
    "    def Hamiltonian(self, v, h): \n",
    "        ah = torch.dot(self.a, h)\n",
    "        bv = torch.dot(self.b, v)\n",
    "        hWv = torch.dot(h, torch.mv(self.W, v))\n",
    "        H = - ah - bv - hWv\n",
    "        return H\n",
    "    \n",
    "    def FreeEnergy(self, v):\n",
    "        bv = torch.dot(self.b, v)\n",
    "        Wv = torch.mv(self.W, v)\n",
    "        F = - bv\n",
    "        for i in range(Wv.size()[0]):\n",
    "            F -= torch.log(1 + torch.exp(self.a[i] + Wv[i]))\n",
    "        return F\n",
    "    \n",
    "    # Calculate p(v = D[i]) using Softmax\n",
    "    def p_v(self, Batch):\n",
    "        # Free Energies of each v = D[i]\n",
    "        batch_size = Batch.size()[0]\n",
    "        F = torch.zeros(batch_size)\n",
    "        for i in range(batch_size):\n",
    "            F[i] = self.FreeEnergy(Batch[i]).item()\n",
    "            \n",
    "        # p(v = D[i]) = Softmax(-F)[i] = exp(-F[i])/Z\n",
    "        p_v = Fcn.softmax(-F, dim=0)\n",
    "        return p_v\n",
    "    \n",
    "    # Calculate Negative Log-Likelihood using log_softmax\n",
    "    def NLL(self, D):\n",
    "        # Free Energies of each v = D[i]\n",
    "        F = torch.zeros(D.size()[0])\n",
    "        for i in range(D.size()[0]):\n",
    "            F[i] = self.FreeEnergy(D[i]).item()\n",
    "            \n",
    "        # p(v = D[i]) = Softmax(-F)[i] = exp(-F[i])/Z\n",
    "        LSM = Fcn.log_softmax(-F, dim=0)\n",
    "        NLL = - torch.mean(LSM)\n",
    "        return NLL\n",
    "    \n",
    "    def sigmoid_i(self, Batch, idx):\n",
    "        a = self.a\n",
    "        Wv = torch.mv(self.W, Batch[idx])\n",
    "        sigmoid = torch.sigmoid(a + Wv)\n",
    "        return sigmoid\n",
    "    \n",
    "    def grad_F_i(self, Batch, idx, param):\n",
    "        \n",
    "        if param == 'W':\n",
    "            grad_F_i = torch.zeros_like(self.W)\n",
    "            for j in range(grad_F_i.size()[0]):\n",
    "                for k in range(grad_F_i.size()[1]):\n",
    "                    grad_F_i[j,k] = - self.sigmoid_i(Batch, idx)[j] * Batch[idx][k]\n",
    "                    \n",
    "        elif param == 'a':\n",
    "            grad_F_i = - self.sigmoid_i(Batch, idx)\n",
    "            \n",
    "        elif param == 'b':\n",
    "            grad_F_i = - Batch[idx]\n",
    "        \n",
    "        return grad_F_i\n",
    "        \n",
    "    # Gradients of Negative Log-Likelihood\n",
    "    def grad_NLL(self, Batch, param):\n",
    "        \n",
    "        if param == 'W':\n",
    "            grad_NLL = torch.zeros_like(self.W)\n",
    "        elif param == 'a':\n",
    "            grad_NLL = torch.zeros_like(self.a)\n",
    "        elif param == 'b':\n",
    "            grad_NLL = torch.zeros_like(self.b)\n",
    "        \n",
    "        batch_size = Batch.size()[0]\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            grad_NLL += (1 / batch_size - self.p_v(Batch)[idx]) * self.grad_F_i(Batch, idx, param)\n",
    "        \n",
    "        return grad_NLL\n",
    "    \n",
    "    # Update the RBM parameters\n",
    "    def update(self, Batch, learning_rate):\n",
    "\n",
    "        grad_NLL_w = self.grad_NLL(Batch, 'W')\n",
    "        grad_NLL_a = self.grad_NLL(Batch, 'a')\n",
    "        grad_NLL_b = self.grad_NLL(Batch, 'b')\n",
    "        \n",
    "        self.W -= learning_rate * grad_NLL_w\n",
    "        self.a -= learning_rate * grad_NLL_a\n",
    "        self.b -= learning_rate * grad_NLL_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to create a training data set by using Metropolis algorithm\n",
    "def Data_for_train(data_size, nRow, nCol):\n",
    "\n",
    "    for i in range(data_size):\n",
    "    \n",
    "        ising = Ising(nRow, nCol)\n",
    "        H_new = ising.Hamiltonian()\n",
    "    \n",
    "        # Reshape of a matrix of Ising spins to a vector as the visible layer\n",
    "        spin = ising.spins.view(nRow*nCol)\n",
    "        v = (1 - spin)/2   # spin 1 --> 0 ,   spin -1 --> 1\n",
    "    \n",
    "        # save visible layers as row vectors of the training data matrix\n",
    "        if i == 0:\n",
    "            data = v.unsqueeze(0)\n",
    "        else:\n",
    "            if H_new <= H:\n",
    "                data = torch.cat((data, v.unsqueeze(0)), dim = 0)\n",
    "            else:\n",
    "                B_sample = torch.bernoulli(torch.exp(H - H_new))\n",
    "                if B_sample == 1:\n",
    "                    data = torch.cat((data, v.unsqueeze(0)), dim = 0)\n",
    "                else:\n",
    "                    data = torch.cat((data, data[i-1].unsqueeze(0)), dim = 0)\n",
    "        H = H_new\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_binary_tensor(value, width=0):\n",
    "    string = format(value, '0{}b'.format(width))\n",
    "    binary = [0 if c == '0' else 1 for c in string]\n",
    "    return torch.tensor(binary, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy_data(Data):\n",
    "    data_size = Data.size()[0]\n",
    "    data_length = Data.size()[1]\n",
    "    num_state = 2**data_length\n",
    "    count = torch.zeros(num_state)\n",
    "    for idx_state in range(num_state):\n",
    "        for idx_data in range(data_size):\n",
    "            bin_state = decimal_to_binary_tensor(idx_state, width=data_length)\n",
    "            if torch.all(torch.eq(Data[idx_data], bin_state)) == 1:\n",
    "                count[idx_state] += 1\n",
    "    \n",
    "    prob = count / data_size\n",
    "    \n",
    "    Entropy = 0\n",
    "    for idx_state in range(num_state):\n",
    "        if prob[idx_state] > 0:\n",
    "            Entropy -= prob[idx_state] * torch.log(prob[idx_state])\n",
    "            \n",
    "    return Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initiate the data set and RBM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9743)\n"
     ]
    }
   ],
   "source": [
    "# make a training data set\n",
    "nRow = 3\n",
    "nCol = 3\n",
    "data_size = 100\n",
    "data_length = nRow*nCol\n",
    "\n",
    "D = Data_for_train(data_size, nRow, nCol)\n",
    "S = Entropy_data(D)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RBM parameters\n",
    "nv = data_length\n",
    "nh = 10\n",
    "rbm = RBM(nv, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 4.922558784484863\n",
      "epoch 1: loss = 2.1298367977142334\n",
      "epoch 2: loss = 1.1264026165008545\n",
      "epoch 3: loss = 0.8990771770477295\n",
      "epoch 4: loss = 0.8246471881866455\n",
      "epoch 5: loss = 0.791759729385376\n",
      "epoch 6: loss = 0.7740018367767334\n",
      "epoch 7: loss = 0.7627289295196533\n",
      "epoch 8: loss = 0.7545626163482666\n",
      "epoch 9: loss = 0.7480294704437256\n",
      "epoch 10: loss = 0.7424390316009521\n"
     ]
    }
   ],
   "source": [
    "# Train the RBM\n",
    "num_epoch = 10\n",
    "batch_size = 10\n",
    "learning_rate = 1e-1\n",
    "\n",
    "for epoch in range(0, num_epoch + 1):\n",
    "    if epoch > 0:\n",
    "        for batch_idx in range(int(data_size / batch_size)):\n",
    "            Batch = D[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "            rbm.update(Batch, learning_rate)\n",
    "        \n",
    "        loss_new = rbm.NLL(D) - S\n",
    "        if (loss - loss_new) < 0.001 and loss_new > S * 0.05:\n",
    "            rbm = RBM(nv, nh)\n",
    "     \n",
    "        loss = loss_new\n",
    "    \n",
    "    else:\n",
    "        loss = rbm.NLL(D) - S\n",
    "        \n",
    "    print('epoch {}: loss = {}'.format(epoch, loss))\n",
    "#    print('\\t W = {}'.format(rbm.W))\n",
    "#    print('\\t a = {}'.format(rbm.a))\n",
    "#    print('\\t b = {}'.format(rbm.b))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
