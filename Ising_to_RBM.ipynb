{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import Ising2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_binary_tensor(value, width=0):\n",
    "    string = format(value, '0{}b'.format(width))\n",
    "    binary = [0 if c == '0' else 1 for c in string]\n",
    "    return torch.tensor(binary, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ising_model():\n",
    "    \n",
    "    # initiate the size of 2d lattice, the (inverse-)temperature, interactions\n",
    "    def __init__(self, num_rows, num_cols, beta):\n",
    "        self.num_rows, self.num_cols = num_rows, num_cols\n",
    "        self.num_spins = num_rows * num_cols\n",
    "        self.beta = beta\n",
    "        self.J = self.interactions()\n",
    "    \n",
    "    # find nearest neighbors in 2d lattice\n",
    "    def neighbors(self, i, j):\n",
    "        nhb = []\n",
    "        if i > 0:\n",
    "            nhb.append([i-1, j])\n",
    "        if i < self.num_rows - 1:\n",
    "            nhb.append([i+1, j])\n",
    "        if j > 0:\n",
    "            nhb.append([i, j-1])\n",
    "        if j < self.num_cols - 1:\n",
    "            nhb.append([i, j+1])\n",
    "        return nhb\n",
    "    \n",
    "    # ferromagnetic interactions only with nearest neighbors\n",
    "    def interactions(self):\n",
    "        J = torch.zeros(self.num_spins, self.num_spins)\n",
    "        for i in range(self.num_rows):\n",
    "            for j in range(self.num_cols):\n",
    "                for i_nhb, j_nhb in self.neighbors(i, j):\n",
    "                    J[self.num_rows * i + j, self.num_rows * i_nhb + j_nhb] = 1                \n",
    "        return J\n",
    "    \n",
    "    # calculate Hamiltonian of Ising model for a given spin state\n",
    "    def hamiltonian(self, spin_state):\n",
    "        s = spin_state\n",
    "        J = self.J\n",
    "        H = - torch.matmul(s, torch.matmul(J, s)) / 2\n",
    "        return H\n",
    "    \n",
    "    # calculate the expectation value of the Energy of Ising model with Boltzmann distribution\n",
    "    def energy_expectation(self):\n",
    "        num_state = 2**(self.num_spins)\n",
    "        Z = 0.\n",
    "        E_true = 0.\n",
    "        for i_state in range(num_state):\n",
    "            binary = decimal_to_binary_tensor(i_state, width=self.num_spins)\n",
    "            spin_state = 1 - 2 * binary\n",
    "            H = self.hamiltonian(spin_state)\n",
    "            Z += torch.exp(-self.beta * H)\n",
    "        for i_state in range(num_state):\n",
    "            binary = decimal_to_binary_tensor(i_state, width=self.num_spins)\n",
    "            spin_state = 1 - 2 * binary\n",
    "            H = self.hamiltonian(spin_state)\n",
    "            E_true += H * torch.exp(- self.beta * H) / Z\n",
    "        return E_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### made by Chae-Yeon\n",
    "class IsingSampler:\n",
    "    def __init__(self, n, m, beta):\n",
    "        self.model = Ising2D.Ising2D(n,m)\n",
    "        self.sampler = Ising2D.WolffSampler(self.model, beta)\n",
    "        self.sampler.set_seed(random.randint(0, 1000))\n",
    "        self.sampler.randomize_conf()\n",
    "\n",
    "        for i in range(30):\n",
    "            self.sampler.sweep()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        res = []\n",
    "        for i in range(batch_size):\n",
    "            res.append(list(self.sampler.conf))\n",
    "            self.sampler.sweep()\n",
    "        return (torch.tensor(res, dtype=torch.float32) + 1.0)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ising_sampler():\n",
    "    \n",
    "    # initiate the size of a dataset, and create a sampled dataset\n",
    "    def __init__(self, data_size, num_rows, num_cols):\n",
    "        self.model = Ising_model(num_rows, num_cols)\n",
    "        self.data_size = data_size\n",
    "        self.data_length = num_rows * num_cols\n",
    "        self.num_rows, self.num_cols = num_rows, num_cols\n",
    "        self.dataset = self.metropolis_sampling()\n",
    "    \n",
    "    # randomly generate one sample of an spin state as 2d tensor\n",
    "    def gen_sample(self, p = 0.5):\n",
    "        probs = torch.rand(self.num_rows, self.num_cols)\n",
    "        sample = torch.where(probs < p, torch.zeros(1), torch.ones(1))\n",
    "        return sample\n",
    "    \n",
    "    # create a training data set by using Metropolis algorithm\n",
    "    def metropolis_sampling(self):\n",
    "        beta = self.model.beta\n",
    "        num_burn = 10000\n",
    "        # randomly sample a ising model\n",
    "        sample_2d = self.gen_sample()\n",
    "        # reshape a 2d sample to a 1d tensor as the visible layer\n",
    "        v_current = sample_2d.view(-1)\n",
    "        # transform the sample to ising spins:  v=0 ---> spin = +1,  v=1 --> spin = -1\n",
    "        spin_state = 1 - 2 * v_current\n",
    "        # calculate the Hamiltonian of the model\n",
    "        E_current = self.model.hamiltonian(spin_state)\n",
    "        # save the sample of the visible layer to the dataset\n",
    "        dataset = v_current.unsqueeze(0)\n",
    "        \n",
    "        for _ in range(num_burn + self.data_size - 1):\n",
    "            # pick a random site and flip a single spin at the site\n",
    "            v_next = v_current\n",
    "            rand_site = random.randint(0, self.data_length - 1)\n",
    "            v_next[rand_site] = 1 - v_next[rand_site]\n",
    "            spin_state = 1 - 2 * v_next\n",
    "            E_next = self.model.hamiltonian(spin_state)\n",
    "            # accept the next state if it has a lower energy\n",
    "            delta_E = E_next - E_current\n",
    "            acceptance = min(1, torch.exp(- beta * delta_E))\n",
    "            p = torch.rand(1)\n",
    "            if p <= acceptance:\n",
    "                v_current = v_next\n",
    "            else:\n",
    "                v_current = v_current\n",
    "            dataset = torch.cat((dataset, v_current.unsqueeze(0)), dim = 0)\n",
    "            E_current = E_next\n",
    "        \n",
    "        dataset_after_burning = dataset[num_burn : num_burn + self.data_size]\n",
    "        return dataset_after_burning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyze_data():\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.data_size = dataset.size()[0]\n",
    "        self.data_length = dataset.size()[1]\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    # calculate the probability of each samples in the dataset by counting\n",
    "    def prob_data(self):\n",
    "        num_state = 2**self.data_length\n",
    "        count = torch.zeros(num_state)\n",
    "        for i_state in range(num_state):\n",
    "            for i_data in range(self.data_size):\n",
    "                bin_state = decimal_to_binary_tensor(i_state, width=self.data_length)\n",
    "                if torch.all(torch.eq(self.dataset[i_data], bin_state)) == 1:\n",
    "                    count[i_state] += 1\n",
    "        prob = count / self.data_size\n",
    "        return prob\n",
    "    \n",
    "    # calculate the Entropy of the dataset\n",
    "    def entropy_data(self):\n",
    "        num_state = 2**self.data_length\n",
    "        prob = self.prob_data()\n",
    "        entropy = 0.\n",
    "        for i_state in range(num_state):\n",
    "            if prob[i_state] > 0:\n",
    "                entropy -= prob[i_state] * torch.log(prob[i_state])  \n",
    "        return entropy\n",
    "    \n",
    "    # calculate the mean value of the Energy of each samples' in the dataset\n",
    "    def energy_data(self, model):\n",
    "        spinset = 1 - 2 * self.dataset\n",
    "        energy = torch.zeros(self.data_size)\n",
    "        for i_data in range(self.data_size):\n",
    "            energy[i_data] = model.hamiltonian(spinset[i_data]).item()\n",
    "        energy_data = torch.mean(energy)\n",
    "        return energy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM():\n",
    "    \n",
    "    # Initiate the sizes of visible and hidden layers, and RBM parameters(weights, biases)\n",
    "    def __init__(self, nv, nh):\n",
    "        self.nv = nv\n",
    "        self.nh = nh\n",
    "        self.W = torch.randn(nh, nv)\n",
    "        self.a = torch.randn(nh)\n",
    "        self.b = torch.randn(nv)\n",
    "    \n",
    "    # calculate the RBM_Energy for a given visible and hidden layer\n",
    "    def energy(self, v, h): \n",
    "        ah = torch.matmul(self.a, h)\n",
    "        bv = torch.matmul(self.b, v)\n",
    "        hWv = torch.matmul(h, torch.matmul(self.W, v))\n",
    "        E = - ah - bv - hWv\n",
    "        return E\n",
    "    \n",
    "    # calculate the Free Energy for a given visible layer\n",
    "    def free_energy(self, v):\n",
    "        bv = torch.matmul(self.b, v)\n",
    "        Wv = torch.matmul(self.W, v)\n",
    "        sp = nn.Softplus()\n",
    "        F = - bv - torch.sum(sp(self.a + Wv))\n",
    "        return F\n",
    "    \n",
    "    # calculate the partition function of RBM model\n",
    "    def partition_function(self):\n",
    "        num_state = 2**self.nv\n",
    "        Z = 0.\n",
    "        for idx_state in range(num_state):\n",
    "            v = decimal_to_binary_tensor(idx_state, width=self.nv)\n",
    "            Z += torch.exp(-self.free_energy(v))\n",
    "        return Z\n",
    "    \n",
    "    # calculate the Negative Log-Likelihood\n",
    "    def nll(self, dataset):\n",
    "        data_size = dataset.size()[0]\n",
    "        Z = self.partition_function()        \n",
    "        F = torch.zeros(data_size)\n",
    "        for i in range(data_size):\n",
    "            F[i] = self.free_energy(dataset[i]).item()\n",
    "        nll = torch.mean(F) + torch.log(Z)\n",
    "        return nll\n",
    "    \n",
    "    def to_hidden(self, visible):\n",
    "        a = self.a\n",
    "        Wv = torch.matmul(self.W, visible)\n",
    "        return torch.sigmoid(a + Wv)\n",
    "\n",
    "    def to_visible(self, hidden):\n",
    "        b = self.b\n",
    "        hW = torch.matmul(hidden, self.W)\n",
    "        return torch.sigmoid(b + hW)\n",
    "    \n",
    "    # update parameters by gradient descent with Contrastive Divergence\n",
    "    def update_cd(self, batch, learning_rate):\n",
    "        batch_size = batch.size()[0]\n",
    "        delta_W = torch.zeros_like(self.W)\n",
    "        delta_a = torch.zeros_like(self.a)\n",
    "        delta_b = torch.zeros_like(self.b)\n",
    "        for i in range(batch_size):\n",
    "            a = self.a\n",
    "            b = self.b\n",
    "            num_iterations = 100\n",
    "            \n",
    "            # k-iterations of Gibbs sampling\n",
    "            for k in range(num_iterations):\n",
    "                if k == 0:\n",
    "                    v0 = batch[i]\n",
    "                    p_h0 = self.to_hidden(v0)\n",
    "                    h0 = torch.bernoulli(p_h0)\n",
    "                    h_old = h0\n",
    "                else:\n",
    "                    h_old = h_new\n",
    "                \n",
    "                p_v = self.to_visible(h_old)\n",
    "                v_new = torch.bernoulli(p_v)\n",
    "                p_h = self.to_hidden(v_new)\n",
    "                h_new = torch.bernoulli(p_h)\n",
    "            \n",
    "            # update parameters after k-iterations of Gibbs sampling\n",
    "            delta_W += learning_rate * (torch.ger(p_h0, v0) - torch.ger(p_h, v_new))\n",
    "            delta_a += learning_rate * (p_h0 - p_h)\n",
    "            delta_b += learning_rate * (v0 - v_new)\n",
    "            \n",
    "        self.W += delta_W / batch_size\n",
    "        self.a += delta_a / batch_size\n",
    "        self.b += delta_b / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ising model parameters\n",
    "num_rows = 2\n",
    "num_cols = 2\n",
    "beta = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set RBM parameters\n",
    "nv = num_rows * num_cols\n",
    "nh = 10\n",
    "rbm = RBM(nv, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_model = -3.6016509532928467\n",
      "E_data = -3.6059999465942383\n",
      "S_data = 1.1912140846252441\n",
      "epoch 0: loss = 2.1371192932128906\n"
     ]
    }
   ],
   "source": [
    "# set training parameters\n",
    "num_epoch = 1000\n",
    "data_size = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# calculate true energy of Ising model\n",
    "model = Ising_model(num_rows, num_cols, beta)\n",
    "E_model = model.energy_expectation()\n",
    "print('E_model = {}'.format(E_model))\n",
    "\n",
    "# sample dataset for training\n",
    "sampler = IsingSampler(num_rows, num_cols, beta)\n",
    "D = sampler.sample(data_size)\n",
    "anal = Analyze_data(D)\n",
    "E_data = anal.energy_data(model)\n",
    "print('E_data = {}'.format(E_data))\n",
    "S_data = anal.entropy_data()\n",
    "print('S_data = {}'.format(S_data))\n",
    "\n",
    "# train the RBM parameters\n",
    "for epoch in range(0, num_epoch + 1):\n",
    "    if epoch > 0:\n",
    "        for batch_idx in range(int(data_size / batch_size)):\n",
    "            batch = D[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "            rbm.update_cd(batch, learning_rate)\n",
    "    loss = rbm.nll(D) - S_data\n",
    "    if epoch%100 == 0:\n",
    "        print('epoch {}: loss = {}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 1.])\n",
      "tensor([0., 0., 1., 0.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([0., 0., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# sampling after training\n",
    "num_sample = 10\n",
    "num_iteration = 1000\n",
    "for k in range(num_sample * num_iteration):\n",
    "    if k == 0:\n",
    "        v_0 = torch.bernoulli(torch.rand(nv))\n",
    "        Wv = torch.matmul(rbm.W, v_0)\n",
    "        p_h_given_v_0 = torch.sigmoid(rbm.a + Wv)\n",
    "        h_0 = torch.bernoulli(p_h_given_v_0)\n",
    "        h_old = h_0\n",
    "    else:\n",
    "        h_old = h_new\n",
    "                \n",
    "    hW = torch.matmul(torch.t(rbm.W), h_old)\n",
    "    p_v_given_h = torch.sigmoid(rbm.b + hW)\n",
    "    v_new = torch.bernoulli(p_v_given_h)\n",
    "    Wv = torch.matmul(rbm.W, v_new)\n",
    "    p_h_given_v = torch.sigmoid(rbm.a + Wv)\n",
    "    h_new = torch.bernoulli(p_h_given_v)\n",
    "    \n",
    "    if k % num_iteration == num_iteration - 1:\n",
    "        print(v_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
