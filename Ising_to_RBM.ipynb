{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fcn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_binary_tensor(value, width=0):\n",
    "    string = format(value, '0{}b'.format(width))\n",
    "    binary = [0 if c == '0' else 1 for c in string]\n",
    "    return torch.tensor(binary, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Ising spins and Calculating Hamiltonian of the Ising system\n",
    "class Ising_model():\n",
    "    \n",
    "    def __init__(self, num_rows, num_cols):\n",
    "        self.num_rows, self.num_cols = num_rows, num_cols\n",
    "        self.num_spins = num_rows * num_cols\n",
    "        self.beta = 1.\n",
    "        self.J = self.magnetization()\n",
    "    \n",
    "    def neighbors(self, i, j):\n",
    "        nhb = []\n",
    "        if i > 0:\n",
    "            nhb.append([i-1, j])\n",
    "        if i < self.num_rows - 1:\n",
    "            nhb.append([i+1, j])\n",
    "        if j > 0:\n",
    "            nhb.append([i, j-1])\n",
    "        if j < self.num_cols - 1:\n",
    "            nhb.append([i, j+1])\n",
    "        return nhb\n",
    "                    \n",
    "    def magnetization(self):\n",
    "        J = torch.zeros(self.num_spins, self.num_spins)\n",
    "        for i in range(self.num_rows):\n",
    "            for j in range(self.num_cols):\n",
    "                for i_nhb, j_nhb in self.neighbors(i, j):\n",
    "                    J[self.num_rows * i + j, self.num_rows * i_nhb + j_nhb] = 1                \n",
    "        return J\n",
    "    \n",
    "    def hamiltonian(self, spin_state):\n",
    "        s = spin_state\n",
    "        J = self.J\n",
    "        H = - torch.matmul(s, torch.matmul(J, s)) / 2\n",
    "        return H\n",
    "    \n",
    "    def Energy_expectation(self):\n",
    "        num_state = 2**(self.num_spins)\n",
    "        Z = 0.\n",
    "        E_true = 0.\n",
    "        for i_state in range(num_state):\n",
    "            binary = decimal_to_binary_tensor(i_state, width=self.num_spins)\n",
    "            spin_state = 1 - 2 * binary\n",
    "            H = self.hamiltonian(spin_state)\n",
    "            Z += torch.exp(-self.beta * H)\n",
    "        for i_state in range(num_state):\n",
    "            binary = decimal_to_binary_tensor(i_state, width=self.num_spins)\n",
    "            spin_state = 1 - 2 * binary\n",
    "            H = self.hamiltonian(spin_state)\n",
    "            E_true += H * torch.exp(- self.beta * H) / Z\n",
    "        return E_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ising_sample():\n",
    "    \n",
    "    def __init__(self, data_size, num_rows, num_cols):\n",
    "        self.ising = Ising_model(num_rows, num_cols)\n",
    "        self.data_size = data_size\n",
    "        self.data_length = num_rows * num_cols\n",
    "        self.num_rows, self.num_cols = num_rows, num_cols\n",
    "        self.dataset = self.Metropolis_Sampling()\n",
    "    \n",
    "    def gen_sample(self, p = 0.5):\n",
    "        probs = torch.rand(self.num_rows, self.num_cols)\n",
    "        sample = torch.where(probs < p, torch.zeros(1), torch.ones(1))\n",
    "        return sample\n",
    "    \n",
    "    # create a training data set by using Metropolis algorithm\n",
    "    def Metropolis_Sampling(self):\n",
    "        beta = self.ising.beta\n",
    "        for i in range(self.data_size):\n",
    "            # randomly sample a ising model\n",
    "            sample_2d = self.gen_sample()\n",
    "            # reshape a 2d sample to a 1d tensor as the visible layer\n",
    "            v = sample_2d.view(-1)\n",
    "            # transform the sample to ising spins:  0 ---> spin +1,    1 --> spin -1\n",
    "            spin_state = 1 - 2 * v\n",
    "            # calculate the Hamiltonian of the model\n",
    "            H_next = self.ising.hamiltonian(spin_state)\n",
    "        \n",
    "            # Monte Carlo sampling by Metropolis algorithm\n",
    "            if i == 0:\n",
    "                dataset = v.unsqueeze(0)\n",
    "            else:\n",
    "                delta_H = H_next - H_current\n",
    "                if delta_H <= 0:\n",
    "                    dataset = torch.cat((dataset, v.unsqueeze(0)), dim = 0)\n",
    "                else:\n",
    "                    ber = torch.bernoulli(torch.exp(- beta * delta_H))\n",
    "                    if ber == 1:\n",
    "                        dataset = torch.cat((dataset, v.unsqueeze(0)), dim = 0)\n",
    "                    else:\n",
    "                        dataset = torch.cat((dataset, dataset[i-1].unsqueeze(0)), dim = 0)\n",
    "            H_current = H_next\n",
    "        return dataset\n",
    "    \n",
    "    def prob_data(self):\n",
    "        num_state = 2**self.data_length\n",
    "        count = torch.zeros(num_state)\n",
    "        for i_state in range(num_state):\n",
    "            for i_data in range(self.data_size):\n",
    "                bin_state = decimal_to_binary_tensor(i_state, width=self.data_length)\n",
    "                if torch.all(torch.eq(self.dataset[i_data], bin_state)) == 1:\n",
    "                    count[i_state] += 1\n",
    "        prob = count / self.data_size\n",
    "        return prob\n",
    "    \n",
    "    def Entropy_data(self):\n",
    "        num_state = 2**self.data_length\n",
    "        prob = self.prob_data()\n",
    "        Entropy = 0.\n",
    "        for i_state in range(num_state):\n",
    "            if prob[i_state] > 0:\n",
    "                Entropy -= prob[i_state] * torch.log(prob[i_state])  \n",
    "        return Entropy\n",
    "    \n",
    "    def Energy_data(self):\n",
    "        spinset = 1 - 2 * self.dataset\n",
    "        E = torch.zeros(self.data_size)\n",
    "        for i_data in range(self.data_size):\n",
    "            E[i_data] = self.ising.hamiltonian(spinset[i_data]).item()\n",
    "        E_data = torch.mean(E)\n",
    "        return E_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the RBM Architecture (weights, biases)\n",
    "class RBM():\n",
    "    \n",
    "    # Initiate RBM parameters\n",
    "    def __init__(self, nv, nh):\n",
    "        self.nv = nv\n",
    "        self.nh = nh\n",
    "        self.W = torch.randn(nh, nv)\n",
    "        self.a = torch.randn(nh)\n",
    "        self.b = torch.randn(nv)\n",
    "    \n",
    "    def Energy(self, v, h): \n",
    "        ah = torch.matmul(self.a, h)\n",
    "        bv = torch.matmul(self.b, v)\n",
    "        hWv = torch.matmul(h, torch.matmul(self.W, v))\n",
    "        E = - ah - bv - hWv\n",
    "        return E\n",
    "    \n",
    "    def FreeEnergy(self, v):\n",
    "        bv = torch.matmul(self.b, v)\n",
    "        Wv = torch.matmul(self.W, v)\n",
    "        sp = nn.Softplus()\n",
    "        F = - bv - torch.sum(sp(self.a + Wv))\n",
    "        return F\n",
    "    \n",
    "    def Partition_function(self):\n",
    "        num_state = 2**self.nv\n",
    "        Z = 0.\n",
    "        for idx_state in range(num_state):\n",
    "            v = decimal_to_binary_tensor(idx_state, width=self.nv)\n",
    "            Z += torch.exp(-self.FreeEnergy(v))\n",
    "        return Z\n",
    "    \n",
    "    # Calculate Negative Log-Likelihood\n",
    "    def NLL(self, Data):\n",
    "        data_size = Data.size()[0]\n",
    "        Z = self.Partition_function()        \n",
    "        F = torch.zeros(data_size)\n",
    "        for i in range(data_size):\n",
    "            F[i] = self.FreeEnergy(Data[i]).item()\n",
    "        NLL = torch.mean(F) + torch.log(Z)\n",
    "        return NLL\n",
    "    \n",
    "    # Contrastive Divergence\n",
    "    def update_CD(self, Batch, learning_rate):\n",
    "        batch_size = Batch.size()[0]\n",
    "        delta_W = torch.zeros_like(self.W)\n",
    "        delta_a = torch.zeros_like(self.a)\n",
    "        delta_b = torch.zeros_like(self.b)\n",
    "        for i in range(batch_size):\n",
    "            a = self.a\n",
    "            b = self.b\n",
    "            num_iterations = 10\n",
    "            \n",
    "            # k-iterations of Gibbs sampling\n",
    "            for k in range(num_iterations):\n",
    "                if k == 0:\n",
    "                    v_0 = Batch[i]\n",
    "                    Wv = torch.matmul(self.W, v_0)\n",
    "                    p_h_given_v_0 = torch.sigmoid(a + Wv)\n",
    "                    h_0 = torch.bernoulli(p_h_given_v_0)\n",
    "                    h_old = h_0\n",
    "                else:\n",
    "                    h_old = h_new\n",
    "                \n",
    "                hW = torch.matmul(torch.t(self.W), h_old)\n",
    "                p_v_given_h = torch.sigmoid(b + hW)\n",
    "                v_new = torch.bernoulli(p_v_given_h)\n",
    "                Wv = torch.matmul(self.W, v_new)\n",
    "                p_h_given_v = torch.sigmoid(a + Wv)\n",
    "                h_new = torch.bernoulli(p_h_given_v)\n",
    "            \n",
    "            # update parameters after k-iterations of Gibbs sampling\n",
    "            delta_W += learning_rate * (torch.ger(p_h_given_v_0, v_0) - torch.ger(p_h_given_v, v_new))\n",
    "            delta_a += learning_rate * (p_h_given_v_0 - p_h_given_v)\n",
    "            delta_b += learning_rate * (v_0 - v_new)\n",
    "            \n",
    "        self.W += delta_W / batch_size\n",
    "        self.a += delta_a / batch_size\n",
    "        self.b += delta_b / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data size\n",
    "num_rows = 2\n",
    "num_cols = 2\n",
    "data_size = 1000\n",
    "num_spins = num_rows * num_cols\n",
    "\n",
    "# set RBM parameters\n",
    "nv = num_spins\n",
    "nh = 10\n",
    "rbm = RBM(nv, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D = tensor([[0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 1., 1.]])\n",
      "E = -0.972000002861023\n",
      "H = -3.6016509532928467\n"
     ]
    }
   ],
   "source": [
    "sampling = Ising_sample(data_size, num_rows, num_cols)\n",
    "D = sampling.dataset\n",
    "print('D = {}'.format(D))\n",
    "E = sampling.Energy_data()\n",
    "print('E = {}'.format(E))\n",
    "model = Ising_model(num_rows, num_cols)\n",
    "H = model.Energy_expectation()\n",
    "print('H = {}'.format(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 3.2910714149475098\n",
      "epoch 1: loss = 3.2321252822875977\n",
      "epoch 2: loss = 3.072360038757324\n",
      "epoch 3: loss = 3.228456974029541\n",
      "epoch 4: loss = 3.2955567836761475\n",
      "epoch 5: loss = 3.2968311309814453\n",
      "epoch 6: loss = 3.153411388397217\n",
      "epoch 7: loss = 3.069809913635254\n",
      "epoch 8: loss = 3.3044538497924805\n",
      "epoch 9: loss = 3.3686604499816895\n",
      "epoch 10: loss = 3.163126230239868\n",
      "epoch 11: loss = 3.097424268722534\n",
      "epoch 12: loss = 3.195131540298462\n",
      "epoch 13: loss = 2.9905457496643066\n",
      "epoch 14: loss = 3.3045527935028076\n",
      "epoch 15: loss = 3.0767908096313477\n",
      "epoch 16: loss = 3.1711554527282715\n",
      "epoch 17: loss = 3.1516358852386475\n",
      "epoch 18: loss = 3.196458578109741\n",
      "epoch 19: loss = 3.108738899230957\n",
      "epoch 20: loss = 3.258394718170166\n",
      "epoch 21: loss = 3.18953800201416\n",
      "epoch 22: loss = 3.1629414558410645\n",
      "epoch 23: loss = 3.0662074089050293\n",
      "epoch 24: loss = 3.126600980758667\n",
      "epoch 25: loss = 3.0537891387939453\n",
      "epoch 26: loss = 3.211574077606201\n",
      "epoch 27: loss = 2.9959802627563477\n",
      "epoch 28: loss = 3.0176243782043457\n",
      "epoch 29: loss = 3.0092084407806396\n",
      "epoch 30: loss = 3.1559863090515137\n",
      "epoch 31: loss = 3.0254955291748047\n",
      "epoch 32: loss = 2.982860803604126\n",
      "epoch 33: loss = 2.9489803314208984\n",
      "epoch 34: loss = 3.1209309101104736\n",
      "epoch 35: loss = 3.167262554168701\n",
      "epoch 36: loss = 2.9648733139038086\n",
      "epoch 37: loss = 3.0653023719787598\n",
      "epoch 38: loss = 2.8581202030181885\n",
      "epoch 39: loss = 2.944876194000244\n",
      "epoch 40: loss = 3.084214687347412\n",
      "epoch 41: loss = 2.927265167236328\n",
      "epoch 42: loss = 3.1935908794403076\n",
      "epoch 43: loss = 2.9105677604675293\n",
      "epoch 44: loss = 2.95224666595459\n",
      "epoch 45: loss = 3.0992271900177\n",
      "epoch 46: loss = 3.0825819969177246\n",
      "epoch 47: loss = 3.1150643825531006\n",
      "epoch 48: loss = 3.088279962539673\n",
      "epoch 49: loss = 3.1957643032073975\n",
      "epoch 50: loss = 3.0317702293395996\n",
      "epoch 51: loss = 2.96279239654541\n",
      "epoch 52: loss = 3.0598301887512207\n",
      "epoch 53: loss = 2.82570481300354\n",
      "epoch 54: loss = 3.1511640548706055\n",
      "epoch 55: loss = 3.12648868560791\n",
      "epoch 56: loss = 2.94960618019104\n",
      "epoch 57: loss = 2.8223531246185303\n",
      "epoch 58: loss = 2.994904041290283\n",
      "epoch 59: loss = 3.0289621353149414\n",
      "epoch 60: loss = 2.986386299133301\n",
      "epoch 61: loss = 2.797064781188965\n",
      "epoch 62: loss = 2.73140811920166\n",
      "epoch 63: loss = 2.8810667991638184\n",
      "epoch 64: loss = 2.701627016067505\n",
      "epoch 65: loss = 2.9494590759277344\n",
      "epoch 66: loss = 2.9164910316467285\n",
      "epoch 67: loss = 2.948857307434082\n",
      "epoch 68: loss = 2.747865676879883\n",
      "epoch 69: loss = 3.0159926414489746\n",
      "epoch 70: loss = 2.87320876121521\n",
      "epoch 71: loss = 2.8780009746551514\n",
      "epoch 72: loss = 3.067676305770874\n",
      "epoch 73: loss = 2.7958407402038574\n",
      "epoch 74: loss = 2.8657848834991455\n",
      "epoch 75: loss = 2.7522239685058594\n",
      "epoch 76: loss = 2.8070356845855713\n",
      "epoch 77: loss = 3.0412096977233887\n",
      "epoch 78: loss = 2.8637380599975586\n",
      "epoch 79: loss = 2.8501899242401123\n",
      "epoch 80: loss = 2.826630115509033\n",
      "epoch 81: loss = 2.7987430095672607\n",
      "epoch 82: loss = 2.850698709487915\n",
      "epoch 83: loss = 2.769993543624878\n",
      "epoch 84: loss = 2.7848334312438965\n",
      "epoch 85: loss = 2.5817337036132812\n",
      "epoch 86: loss = 2.657029628753662\n",
      "epoch 87: loss = 3.056919574737549\n",
      "epoch 88: loss = 2.839360475540161\n",
      "epoch 89: loss = 2.879188299179077\n",
      "epoch 90: loss = 2.9209346771240234\n",
      "epoch 91: loss = 2.816056728363037\n",
      "epoch 92: loss = 2.720158815383911\n",
      "epoch 93: loss = 2.6831624507904053\n",
      "epoch 94: loss = 2.882812023162842\n",
      "epoch 95: loss = 2.7204442024230957\n",
      "epoch 96: loss = 2.6732444763183594\n",
      "epoch 97: loss = 2.7872235774993896\n",
      "epoch 98: loss = 2.6690385341644287\n",
      "epoch 99: loss = 2.7213165760040283\n",
      "epoch 100: loss = 2.8685879707336426\n"
     ]
    }
   ],
   "source": [
    "# Train the RBM\n",
    "num_epoch = 100\n",
    "#batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for epoch in range(0, num_epoch + 1):\n",
    "    sampling = Ising_sample(data_size, num_rows, num_cols)\n",
    "    D = sampling.dataset\n",
    "    S = sampling.Entropy_data()\n",
    "    if epoch > 0:\n",
    "#        for batch_idx in range(int(data_size / batch_size)):\n",
    "#            Batch = D[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "        rbm.update_CD(D, learning_rate)\n",
    "    loss = rbm.NLL(D) - S\n",
    "    print('epoch {}: loss = {}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 1.])\n",
      "tensor([0., 0., 1., 0.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([0., 1., 1., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 1., 1.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([0., 0., 1., 0.])\n",
      "tensor([1., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# sampling after training\n",
    "for k in range(10000):\n",
    "    if k == 0:\n",
    "        v_0 = torch.bernoulli(torch.rand(nv))\n",
    "        Wv = torch.matmul(rbm.W, v_0)\n",
    "        p_h_given_v_0 = torch.sigmoid(rbm.a + Wv)\n",
    "        h_0 = torch.bernoulli(p_h_given_v_0)\n",
    "        h_old = h_0\n",
    "    else:\n",
    "        h_old = h_new\n",
    "                \n",
    "    hW = torch.matmul(torch.t(rbm.W), h_old)\n",
    "    p_v_given_h = torch.sigmoid(rbm.b + hW)\n",
    "    v_new = torch.bernoulli(p_v_given_h)\n",
    "    Wv = torch.matmul(rbm.W, v_new)\n",
    "    p_h_given_v = torch.sigmoid(rbm.a + Wv)\n",
    "    h_new = torch.bernoulli(p_h_given_v)\n",
    "    \n",
    "    if k%1000==999:\n",
    "        print(v_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
